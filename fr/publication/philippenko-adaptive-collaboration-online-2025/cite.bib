@unpublished{philippenkoAdaptiveCollaborationOnline2025,
 abstract = {We study the problem of online personalized decentralized learning with NNN statistically heterogeneous clients collaborating to accelerate local training. An important challenge in this setting is to select relevant collaborators to reduce gradient variance while mitigating the introduced bias. To tackle this, we introduce a gradient-based collaboration criterion, allowing each client to dynamically select peers with similar gradients during the optimization process. Our criterion is motivated by a refined and more general theoretical analysis of the \ texttt\All-for-one\ algorithm, proved to be optimal in Even et al. (2022) for an oracle collaboration scheme. We derive excess loss upper-bounds for smooth objective functions, being either strongly convex, non-convex, or satisfying the Polyak-≈Åojasiewicz condition; our analysis reveals that the algorithm acts as a variance reduction method where the speed-up depends on a \ emph\sufficient variance\.  We put forward two collaboration methods instantiating the proposed general schema; and we show that one variant preserves the optimality of \ texttt\All-for-one\.  We validate our results with experiments on synthetic and real datasets.},
 author = {Philippenko, Constantin and Le Bars, Batiste and Scaman, Kevin and Massoulie, Laurent},
 date = {2025-07},
 keywords = {Federated Learning,Optimization,Personalization},
 title = {Adaptive Collaboration for Online Personalized Distributed Learning with Heterogeneous Clients},
 url = {https://hal.science/hal-05175097},
 urldate = {2025-10-16}
}
