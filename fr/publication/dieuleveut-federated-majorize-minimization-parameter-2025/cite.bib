@unpublished{dieuleveutFederatedMajorizeMinimizationParameter2025,
 abstract = {This paper proposes a unified approach for designing stochastic optimization algorithms that robustly scale to the federated learning setting. Our work studies a class of Majorize-Minimization (MM) problems, which possesses a linearly parameterized family of majorizing surrogate functions. This framework encompasses (proximal) gradient-based algorithms for (regularized) smooth objectives, the Expectation Maximization algorithm, and many problems seen as variational surrogate MM. We show that our framework motivates a unifying algorithm called Stochastic Approximation Stochastic Surrogate MM (SA-SSMM), which includes previous stochastic MM procedures as special instances. We then extend SA-SSMM to the federated setting, while taking into consideration common bottlenecks such as data heterogeneity, partial participation, and communication constraints; this yields FedMM. The originality of FedMM is to learn locally and then aggregate information characterizing the surrogate majorizing function, contrary to classical algorithms which learn and aggregate the original parameter. Finally, to showcase the flexibility of this methodology beyond our theoretical setting, we use it to design an algorithm for computing optimal transport maps in the federated setting.},
 author = {Dieuleveut, Aymeric and Fort, Gersende and Hegazy, Mahmoud and Wai, Hoi-To},
 date = {2025-07},
 keywords = {Federated Learning,Stochastic Approximation,Stochastic Optimization,Surrogate Methods},
 shorttitle = {Federated Majorize-Minimization},
 title = {Federated Majorize-Minimization: Beyond Parameter Aggregation},
 url = {https://hal.science/hal-05189632},
 urldate = {2025-10-16}
}
