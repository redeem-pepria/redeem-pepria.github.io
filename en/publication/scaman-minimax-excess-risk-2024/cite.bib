@inproceedings{scamanMinimaxExcessRisk2024,
 abstract = {In this paper, our aim is to analyse the generalization capabilities of first-order methods for statistical learning in multiple, different yet related, scenarios including supervised learning, transfer learning, robust learning and federated learning. To do so, we provide sharp upper and lower bounds for the minimax excess risk of strongly convex and smooth statistical learning when the gradient is accessed through partial observations given by a data-dependent oracle. This novel class of oracles can query the gradient with any given data distribution, and is thus well suited to scenarios in which the training data distribution does not match the target (or test) distribution. In particular, our upper and lower bounds are proportional to the smallest mean square error achievable by gradient estimators, thus allowing us to easily derive multiple sharp bounds in the aforementioned scenarios using the extensive literature on parameter estimation.},
 author = {Scaman, Kevin and Even, Mathieu and Bars, Batiste Le and Massoulié, Laurent},
 booktitle = {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
 date = {2024-04-18},
 eventtitle = {International Conference on Artificial Intelligence and Statistics},
 file = {C:\Usersģ228481\Documents\50biblio\zotero\PMLR\Scaman et al_2024_Minimax Excess Risk of First-Order Methods for Statistical Learning with.pdf},
 issn = {2640-3498},
 keywords = {INRIA-ARGO},
 langid = {english},
 pages = {3709--3717},
 publisher = {PMLR},
 title = {Minimax Excess Risk of First-Order Methods for Statistical Learning with Data-Dependent Oracles},
 url = {https://proceedings.mlr.press/v238/scaman24a.html},
 urldate = {2025-01-27}
}
