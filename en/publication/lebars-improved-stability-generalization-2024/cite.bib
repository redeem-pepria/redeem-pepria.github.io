@inproceedings{lebarsImprovedStabilityGeneralization2024,
 abstract = {This paper presents a new generalization error analysis for Decentralized Stochastic Gradient Descent (D-SGD) based on algorithmic stability. The obtained results overhaul a series of recent works that suggested an increased instability due to decentralization and a detrimental impact of poorly-connected communication graphs on generalization. On the contrary, we show, for convex, strongly convex and non-convex functions, that D-SGD can always recover generalization bounds analogous to those of classical SGD, suggesting that the choice of graph does not matter. We then argue that this result is coming from a worst-case analysis, and we provide a refined optimization-dependent generalization bound for general convex functions. This new bound reveals that the choice of graph can in fact improve the worst-case bound in certain regimes, and that surprisingly, a poorly-connected graph can even be beneficial for generalization.},
 author = {Le Bars, Batiste and Bellet, Aurélien and Tommasi, Marc and Scaman, Kevin and Neglia, Giovanni},
 booktitle = {Proceedings of the 41st International Conference on Machine Learning},
 date = {2024-07-21},
 file = {C:\Usersģ228481\Documents\50biblio\zotero\arXiv\Bars et al_2024_Improved Stability and Generalization Guarantees of the Decentralized SGD.pdf},
 keywords = {INRIA-ARGO,INRIA-MAGNET},
 location = {Vienna, Austria},
 pages = {26215--26240},
 publisher = {JMLR.org},
 series = {ICML'24},
 title = {Improved Stability and Generalization Guarantees of the Decentralized SGD Algorithm},
 volume = {235}
}
