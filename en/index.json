
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":["César Sabater"],"categories":null,"content":"","date":1761955200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1761955200,"objectID":"728ed53b6dbd0204893987697b151a8e","permalink":"https://redeem-pepria.github.io/en/author/cesar-sabater/","publishdate":"2025-10-16T21:33:42.623859Z","relpermalink":"/en/author/cesar-sabater/","section":"authors","summary":"","tags":null,"title":"César Sabater","type":"authors"},{"authors":["Julien Nicolas"],"categories":null,"content":"To be completed\n","date":1761955200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1761955200,"objectID":"4ed72accaa85bd5f22c784547a8b63e4","permalink":"https://redeem-pepria.github.io/en/author/julien-nicolas/","publishdate":"2025-10-16T21:33:42.623859Z","relpermalink":"/en/author/julien-nicolas/","section":"authors","summary":"To be completed\n","tags":null,"title":"Julien Nicolas","type":"authors"},{"authors":["Mohamed Maouche"],"categories":null,"content":"Mohamed Maouche is a researcher at Inria Lyon (ISFP) and member of the PRIVATICS team since 2022. His interest is to build robust machine learning systems that manage a good trade-off between privacy, security and utility. This includes working on data anonymization and privacy preserving machine learning with robust aggregation. Previously, he was a one-year post-doc whithin the Chaire DSVD supported by Renault and Labex IMU working on privacy issues in Federated Learning. He also spent two years as a post-doc in MAGNET Team (Inria Lille) to work on speech anonymization. He defended his PhD from Insa Lyon in 2019 on the subject of Location privacy.\n","date":1761955200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1761955200,"objectID":"42f96f22a28b2bde47c757c871c3ea68","permalink":"https://redeem-pepria.github.io/en/author/mohamed-maouche/","publishdate":"2025-10-16T21:33:42.623859Z","relpermalink":"/en/author/mohamed-maouche/","section":"authors","summary":"Mohamed Maouche is a researcher at Inria Lyon (ISFP) and member of the PRIVATICS team since 2022. His interest is to build robust machine learning systems that manage a good trade-off between privacy, security and utility. This includes working on data anonymization and privacy preserving machine learning with robust aggregation. Previously, he was a one-year post-doc whithin the Chaire DSVD supported by Renault and Labex IMU working on privacy issues in Federated Learning. He also spent two years as a post-doc in MAGNET Team (Inria Lille) to work on speech anonymization. He defended his PhD from Insa Lyon in 2019 on the subject of Location privacy.\n","tags":null,"title":"Mohamed Maouche","type":"authors"},{"authors":["Sonia Ben Mokhtar"],"categories":null,"content":"I have been a CNRS researcher at the LIRIS lab since October 2009. Since 2017, I am also the leader of the distributed systems and information retrieval group (DRIM). Before joining CNRS, I was a research associate at University College London (UCL) for two years, working with Licia Capra. I received my PhD in 2007 from University Pierre et Marie Curie (Paris 6), which I did under the supervision of Valérie Issarny and Nikolaos Georgantas in the former INRIA ARLES project-team (currently MiMove).\n","date":1761955200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1761955200,"objectID":"8a457e262205130de8c55f859f13c478","permalink":"https://redeem-pepria.github.io/en/author/sonia-ben-mokhtar/","publishdate":"2025-10-16T21:33:42.623859Z","relpermalink":"/en/author/sonia-ben-mokhtar/","section":"authors","summary":"I have been a CNRS researcher at the LIRIS lab since October 2009. Since 2017, I am also the leader of the distributed systems and information retrieval group (DRIM). Before joining CNRS, I was a research associate at University College London (UCL) for two years, working with Licia Capra. I received my PhD in 2007 from University Pierre et Marie Curie (Paris 6), which I did under the supervision of Valérie Issarny and Nikolaos Georgantas in the former INRIA ARLES project-team (currently MiMove).\n","tags":null,"title":"Sonia Ben Mokhtar","type":"authors"},{"authors":["Aymeric Dieuleveut"],"categories":null,"content":"To be completed\n","date":1759708800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1759708800,"objectID":"6072d47e443fd9d457d66f6764bd1988","permalink":"https://redeem-pepria.github.io/en/author/aymeric-dieuleveut/","publishdate":"2025-10-16T21:33:42.528827Z","relpermalink":"/en/author/aymeric-dieuleveut/","section":"authors","summary":"To be completed\n","tags":null,"title":"Aymeric Dieuleveut","type":"authors"},{"authors":["Renaud Gaucher"],"categories":null,"content":"","date":1759708800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1759708800,"objectID":"4b95ff05571850f55de2ef250d31b0bb","permalink":"https://redeem-pepria.github.io/en/author/renaud-gaucher/","publishdate":"2025-10-16T21:33:42.441801Z","relpermalink":"/en/author/renaud-gaucher/","section":"authors","summary":"","tags":null,"title":"Renaud Gaucher","type":"authors"},{"authors":["Jan Ramon"],"categories":null,"content":"To be completed\n","date":1759276800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1759276800,"objectID":"b0458a5182da4ba951057101c34fbffc","permalink":"https://redeem-pepria.github.io/en/author/jan-ramon/","publishdate":"2025-10-16T21:33:42.615137Z","relpermalink":"/en/author/jan-ramon/","section":"authors","summary":"To be completed\n","tags":null,"title":"Jan Ramon","type":"authors"},{"authors":["Cédric Gouy-Pailler"],"categories":null,"content":"Cédric Gouy-Pailler holds an engineering degree in electronics and signal processing from PHELMA (ex-ENSERG), Grenoble-INP, he received a doctorate in signal processing in 2009 at GIPSA-lab (Grenoble INP). Senior Expert at CEA since 2021, he is now leading the Artificial Intelligence and Machine Learning laboratory at CEA LIST. He is working on innovative machine learning algorithms, with the objective of developing robust AI in adversarial contexts, and compatible with high privacy standards, through the use of homomorphic encryption and differential privacy. Cédric Gouy-Pailler has been involved in various National and European projects, involving both academic and industrials partners. He has been technical CEA leader (eCo-fev, EU project, 2012-2015), Work Package leader (STARLIGHT, EU project, 2021-2025, involving 52 partners), and project coordinator (StreamOPS, national 3 partners, 2018-2021 ; KINAITICS, EU project, 7 partners, 2022-2025).\n","date":1758326400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1758326400,"objectID":"e92b45342344fb7285c16b647f500037","permalink":"https://redeem-pepria.github.io/en/author/cedric-gouy-pailler/","publishdate":"2025-10-16T21:33:42.45207Z","relpermalink":"/en/author/cedric-gouy-pailler/","section":"authors","summary":"Cédric Gouy-Pailler holds an engineering degree in electronics and signal processing from PHELMA (ex-ENSERG), Grenoble-INP, he received a doctorate in signal processing in 2009 at GIPSA-lab (Grenoble INP). Senior Expert at CEA since 2021, he is now leading the Artificial Intelligence and Machine Learning laboratory at CEA LIST. He is working on innovative machine learning algorithms, with the objective of developing robust AI in adversarial contexts, and compatible with high privacy standards, through the use of homomorphic encryption and differential privacy. Cédric Gouy-Pailler has been involved in various National and European projects, involving both academic and industrials partners. He has been technical CEA leader (eCo-fev, EU project, 2012-2015), Work Package leader (STARLIGHT, EU project, 2021-2025, involving 52 partners), and project coordinator (StreamOPS, national 3 partners, 2018-2021 ; KINAITICS, EU project, 7 partners, 2022-2025).\n","tags":null,"title":"Cédric Gouy-Pailler","type":"authors"},{"authors":["Pierre Jobic"],"categories":null,"content":"","date":1756080000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1756080000,"objectID":"192f9298747be2ec60c964d0a82ec680","permalink":"https://redeem-pepria.github.io/en/author/pierre-jobic/","publishdate":"2025-10-16T21:33:42.484704Z","relpermalink":"/en/author/pierre-jobic/","section":"authors","summary":"","tags":null,"title":"Pierre Jobic","type":"authors"},{"authors":["Batiste Le Bars"],"categories":null,"content":"","date":1751328000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1751328000,"objectID":"17f3d600201225c3a3b2111960f0e6ae","permalink":"https://redeem-pepria.github.io/en/author/batiste-le-bars/","publishdate":"2025-10-16T21:33:42.605588Z","relpermalink":"/en/author/batiste-le-bars/","section":"authors","summary":"","tags":null,"title":"Batiste Le Bars","type":"authors"},{"authors":["Constantin Philippenko"],"categories":null,"content":"I am a postdoctoral researcher at DI ENS and Inria Paris working with Kevin Scaman and Laurent Massoulié.\nI obtained my Ph.D. in “Applied Mathematics and Computer Science” from École Polytechnique (Paris) under the joint supervision of Eric Moulines and Aymeric Dieuleveut, professors at the CMAP laboratory of École polytechnique.\nThe goal of the thesis is to focus simultaneously on two challenges of federated learning (i.e. machine learning distributed across millions of devices):\nreducing the cost of communication by doing bidirectional compression, while considering a heterogeneous setting. I graduated from Ensimag (Grenoble) with a specialisation in computer science, applied mathematics and 3D animation.\nI also had the opportunity to do a six-month academic exchange with the MIPT (Moscow) to study statistics, machine learning and image recognition under the supervision of Vadim Strijov.\n","date":1751328000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1751328000,"objectID":"1cad88896e6223d42c2019afb6c7c50c","permalink":"https://redeem-pepria.github.io/en/author/constantin-philippenko/","publishdate":"2025-10-16T21:33:42.588434Z","relpermalink":"/en/author/constantin-philippenko/","section":"authors","summary":"I am a postdoctoral researcher at DI ENS and Inria Paris working with Kevin Scaman and Laurent Massoulié.\nI obtained my Ph.D. in “Applied Mathematics and Computer Science” from École Polytechnique (Paris) under the joint supervision of Eric Moulines and Aymeric Dieuleveut, professors at the CMAP laboratory of École polytechnique.\n","tags":null,"title":"Constantin Philippenko","type":"authors"},{"authors":["Kevin Scaman"],"categories":null,"content":"To be completed\n","date":1751328000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1751328000,"objectID":"8478952da8934ada6c3295340c345d1b","permalink":"https://redeem-pepria.github.io/en/author/kevin-scaman/","publishdate":"2025-10-16T21:33:42.605588Z","relpermalink":"/en/author/kevin-scaman/","section":"authors","summary":"To be completed\n","tags":null,"title":"Kevin Scaman","type":"authors"},{"authors":["Mahmoud Hegazy"],"categories":null,"content":"","date":1751328000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1751328000,"objectID":"79e57059a48ba426f390012c572d8e76","permalink":"https://redeem-pepria.github.io/en/author/mahmoud-hegazy/","publishdate":"2025-10-16T21:33:42.462415Z","relpermalink":"/en/author/mahmoud-hegazy/","section":"authors","summary":"","tags":null,"title":"Mahmoud Hegazy","type":"authors"},{"authors":["Rebecca Clain"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1743465600,"objectID":"f8b93aaeb8f6ab80ca92adc511cbc7c0","permalink":"https://redeem-pepria.github.io/en/author/rebecca-clain/","publishdate":"2025-10-16T21:33:42.352321Z","relpermalink":"/en/author/rebecca-clain/","section":"authors","summary":"","tags":null,"title":"Rebecca Clain","type":"authors"},{"authors":["Brandon A. Mosqueda González"],"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1735689600,"objectID":"11791f3594bf992ef830d352b2823ec3","permalink":"https://redeem-pepria.github.io/en/author/brandon-a.-mosqueda-gonzalez/","publishdate":"2025-01-28T21:08:55.802492Z","relpermalink":"/en/author/brandon-a.-mosqueda-gonzalez/","section":"authors","summary":"","tags":null,"title":"Brandon A. Mosqueda González","type":"authors"},{"authors":["Marc Tommasi"],"categories":null,"content":"","date":1721520000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1721520000,"objectID":"6b1f433e1ec5b8fb73fbbceeaaaf69d3","permalink":"https://redeem-pepria.github.io/en/author/marc-tommasi/","publishdate":"2025-10-16T21:33:42.506566Z","relpermalink":"/en/author/marc-tommasi/","section":"authors","summary":"","tags":null,"title":"Marc Tommasi","type":"authors"},{"authors":["Michaël Perrot"],"categories":null,"content":"","date":1707091200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1707091200,"objectID":"6fe4acb046666c5def283215500ac480","permalink":"https://redeem-pepria.github.io/en/author/michael-perrot/","publishdate":"2025-10-16T21:33:42.395418Z","relpermalink":"/en/author/michael-perrot/","section":"authors","summary":"","tags":null,"title":"Michaël Perrot","type":"authors"},{"authors":["Alexandre Rapetti"],"categories":null,"content":"Alexandre Rapetti has been a researcher at CEA LIST since 2024. He completed his PhD in Computer Science at Aix-Marseille University in 2023, conducted at CEA LIST under the supervision of Alessia Milani, Professor at Aix-Marseille University, and the guidance of Antonella Del Pozzo, Researcher at CEA LIST. His research focuses on Distributed Computing, with a particular interest in Byzantine, Fault Tolerance and Privacy-Preserving techniques.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"52c79d5445155550e68fe7ab60596996","permalink":"https://redeem-pepria.github.io/en/author/alexandre-rapetti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/alexandre-rapetti/","section":"authors","summary":"Alexandre Rapetti has been a researcher at CEA LIST since 2024. He completed his PhD in Computer Science at Aix-Marseille University in 2023, conducted at CEA LIST under the supervision of Alessia Milani, Professor at Aix-Marseille University, and the guidance of Antonella Del Pozzo, Researcher at CEA LIST. His research focuses on Distributed Computing, with a particular interest in Byzantine, Fault Tolerance and Privacy-Preserving techniques.\n","tags":null,"title":"Alexandre Rapetti","type":"authors"},{"authors":["Alice Batté"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"03828da16ce8923f2f03c4c116dfac23","permalink":"https://redeem-pepria.github.io/en/author/alice-batte/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/alice-batte/","section":"authors","summary":"","tags":null,"title":"Alice Batté","type":"authors"},{"authors":["Antonella Del Pozzo"],"categories":null,"content":"To be completed\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"16ffc94c4f7436ef6da318dc0623bb89","permalink":"https://redeem-pepria.github.io/en/author/antonella-del-pozzo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/antonella-del-pozzo/","section":"authors","summary":"To be completed\n","tags":null,"title":"Antonella Del Pozzo","type":"authors"},{"authors":["Arnaud Descours"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4b325dd492b3e3a337a776a91ba98b5c","permalink":"https://redeem-pepria.github.io/en/author/arnaud-descours/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/arnaud-descours/","section":"authors","summary":"","tags":null,"title":"Arnaud Descours","type":"authors"},{"authors":["Baptiste Goujaud"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d5bcb2c1a4d1d3a9e84b6e9589778aec","permalink":"https://redeem-pepria.github.io/en/author/baptiste-goujaud/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/baptiste-goujaud/","section":"authors","summary":"","tags":null,"title":"Baptiste Goujaud","type":"authors"},{"authors":["Jean-Baptiste Fest"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6d903135b03e6cf8a9ef3f5c19e79254","permalink":"https://redeem-pepria.github.io/en/author/jean-baptiste-fest/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/jean-baptiste-fest/","section":"authors","summary":"","tags":null,"title":"Jean-Baptiste Fest","type":"authors"},{"authors":["Mathieu Gestin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8ee49ac702a599a0cd7cbf693ff36495","permalink":"https://redeem-pepria.github.io/en/author/mathieu-gestin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/mathieu-gestin/","section":"authors","summary":"","tags":null,"title":"Mathieu Gestin","type":"authors"},{"authors":["Oudom Kem"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"92c8362f87b5154234592c92c9707833","permalink":"https://redeem-pepria.github.io/en/author/oudom-kem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/oudom-kem/","section":"authors","summary":"","tags":null,"title":"Oudom Kem","type":"authors"},{"authors":["Julien Nicolas","César Sabater","Mohamed Maouche","Mark Coates","Sonia Ben Mokhtar"],"categories":null,"content":"","date":1761955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1761955200,"objectID":"5fbf1b6adb6b8c0e83a6cca3fa404ce1","permalink":"https://redeem-pepria.github.io/en/publication/nicolas-privi-rec-confidential-decentralized-2025/","publishdate":"2025-10-16T21:33:42.558007Z","relpermalink":"/en/publication/nicolas-privi-rec-confidential-decentralized-2025/","section":"publication","summary":"Recent advances in recommender systems have shown that relying on graph filters, such as the normalized item-item adjacency matrix and the ideal low-pass filter yields competitive performance and scales better than Graph Convolutional Networks-based solutions. However, these solutions require centralizing user data, which raises concerns over data privacy, security, and the monopolization of user data by a few actors. To address those concerns, we propose PriviRec and PriviRec-k, two complementary recommendation frameworks. In PriviRec, we show that it is possible to decompose widely used filters so that they can be computed in a distributed setting using Secure Aggregation and a distributed version of the Randomized Power Method, without revealing individual users contributions. PriviRec-k extends this approach by having users securely aggregate low-rank projections of their contributions, enabling a tunable balance between communication overhead and recommendation accuracy. We demonstrate theoretically as well as experimentally on Gowalla, Yelp2018, and Amazon-Book that our methods achieve performance comparable to centralized state-of-the-art recommender systems and superior to decentralized ones, while preserving confidentiality and low communication and computational overheads.","tags":["BSPM","decentralized","distributed","GF-CF","privacy","Secure Aggregation"],"title":"PriviRec: Confidential and Decentralized Graph Filtering for Recommender Systems","type":"publication"},{"authors":["Renaud Gaucher","Aymeric Dieuleveut","Hadrien Hendrikx"],"categories":null,"content":"","date":1759708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1759708800,"objectID":"8d710b5321b1cafc2d7af8576c5c6a4b","permalink":"https://redeem-pepria.github.io/en/publication/gaucher-unified-breakdown-analysis-2025/","publishdate":"2025-10-16T21:33:42.441801Z","relpermalink":"/en/publication/gaucher-unified-breakdown-analysis-2025/","section":"publication","summary":"In decentralized machine learning, different devices communicate in a peer-to-peer manner to collaboratively learn from each other’s data. Such approaches are vulnerable to misbehaving (or Byzantine) devices. We introduce F-RG, a general framework for building robust decentralized algorithms with guarantees arising from robust-sum-like aggregation rules F. We then investigate the notion of breakdown point, and show an upper bound on the number of adversaries that decentralized algorithms can tolerate. We introduce a practical robust aggregation rule, coined CS+, such that CS+-RG has a near-optimal breakdown. Other choices of aggregation rules lead to existing algorithms such as ClippedGossip or NNA. We give experimental evidence to validate the effectiveness of CS+-RG and highlight the gap with NNA, in particular against a novel attack tailored to decentralized communications.","tags":null,"title":"Unified Breakdown Analysis for Byzantine Robust Gossip","type":"publication"},{"authors":["César Sabater","Sonia Ben Mokhtar","Jan Ramon"],"categories":null,"content":"","date":1759276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1759276800,"objectID":"07ed3f51ace90a785cd412721b0fac23","permalink":"https://redeem-pepria.github.io/en/publication/sabater-dropout-robust-mechanisms-differentially-2025/","publishdate":"2025-10-16T21:33:42.596393Z","relpermalink":"/en/publication/sabater-dropout-robust-mechanisms-differentially-2025/","section":"publication","summary":"Achieving differentially private computations in decentralized settings poses significant challenges, particularly regarding accuracy, communication cost, and robustness against information leakage. While cryptographic solutions offer promise, they often suffer from high communication overhead or require centralization in the presence of network failures. Conversely, existing fully decentralized approaches typically rely on relaxed adversarial models or pairwise noise cancellation, the latter suffering from substantial accuracy degradation if parties unexpectedly disconnect. In this work, we propose IncA, a new protocol for fully decentralized mean estimation, a widely used primitive in data-intensive processing. Our protocol, which enforces differential privacy, requires no central orchestration and employs low-variance correlated noise, achieved by incrementally injecting sensitive information into the computation. First, we theoretically demonstrate that, when no parties permanently disconnect, our protocol achieves accuracy comparable to that of a centralized setting-already an improvement over most existing decentralized differentially private techniques. Second, we empirically show that our use of low-variance correlated noise significantly mitigates the accuracy loss experienced by existing techniques in the presence of dropouts.","tags":["decentralized mean estimation","decentralized optimization","differential privacy"],"title":"Dropout-Robust Mechanisms for Differentially Private and Fully Decentralized Mean Estimation","type":"publication"},{"authors":["Marc Damie","Edwige Cyffers"],"categories":null,"content":"","date":1759276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1759276800,"objectID":"531ba05b7593cd17d68f1319980bdf7c","permalink":"https://redeem-pepria.github.io/en/publication/damie-fedivertex-graph-dataset-2025/","publishdate":"2025-10-16T21:33:42.363598Z","relpermalink":"/en/publication/damie-fedivertex-graph-dataset-2025/","section":"publication","summary":"Decentralized machine learning - where each client keeps its own data locally and uses its own computational resources to collaboratively train a model by exchanging peer-to-peer messages - is increasingly popular, as it enables better scalability and control over the data. A major challenge in this setting is that learning dynamics depend on the topology of the communication graph, which motivates the use of real graph datasets for benchmarking decentralized algorithms. Unfortunately, existing graph datasets are largely limited to for-profit social networks crawled at a fixed point in time and often collected at the user scale, where links are heavily influenced by the platform and its recommendation algorithms. The Fediverse, which includes several free and open-source decentralized social media platforms such as Mastodon, Misskey, and Lemmy, offers an interesting real-world alternative. We introduce Fedivertex, a new dataset of 182 graphs, covering seven social networks from the Fediverse, crawled weekly over 14 weeks. We release the dataset along with a Python package to facilitate its use, and illustrate its utility on several tasks, including a new defederation task, which captures a process of link deletion observed on these networks.","tags":["FOS: Computer and information sciences","Machine Learning (cs.LG)","Social and Information Networks (cs.SI)"],"title":"Fedivertex: A Graph Dataset Based on Decentralized Social Networks for Trustworthy Machine Learning","type":"publication"},{"authors":["Cédric Gouy-Pailler"],"categories":null,"content":"We are pleased to announce the REDEEM project retreat taking place in Annecy.\nThis two-day seminar will bring together project members and partners to present, exchange, and discuss ongoing research on decentralized, robust and privacy-preserving learning.\nVenue Centre Jean XXIII\n10 Chemin du Bray\n74940 Annecy-le-Vieux\nProgram Day 1 — September 24, 2025 Time Speaker Title 09:00-09:30 — Arrival 09:30-10:15 Cédric Gouy-Pailler Introduction 10:15-10:45 César Sabater Secure Millon-party Computation of Machine Learning Models 10:45-11:15 — Break 11:15-12:00 Jan Ramon Privacy 12:00-13:30 — Lunch 13:30-14:00 Ousmane Touat Towards Byzantine Resilient and Privacy Preserving Decentralized Learning 14:00-14:30 Johan Leydet Exploiting Nearest Neighbor Mixing: Nearest Neighbor Poisoning 14:30-14:50 Mathieu Gestin Privacy-Preserving Federated Averaging with Byzantine Aggregators and Asynchronous Communication 14:50-15:10 — Break 15:10-15:30 Renaud Gaucher A unified Breakdown analysis of Byzantine Robust Gossip 15:30-15:50 Mohamed Maouche GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework 15:50-16:10 Paul Mangold Federated Reinforcement Learning 16:15-20:00 — Social activities 20:00 — Dinner — Brasserie Saint Maurice, 9 rue du Collège Chapuisien, 74000 Annecy Day 2 — September 25, 2025 Time Speaker Title 9:30-9:50 Hadrien Hendrikx Benchmarks for decentralized optimization 9:50-10:10 Brandon Mosqueda Privacy-preserving decentralized learning with trustlessness: A survey 10:10-10:30 Jean-Baptiste Fest Decentralized Optimization Mean-field Approaches 10:30-10:50 — Break 10:50-11:10 Lucas Versini A Markov Chain Analysis of Stochastic Algorithms in Decentralized Learning 11:10-11:30 Cédric Prigent Efficient Federated Learning Clustering with Local Feature Extraction 11:30-12:50 Cédric Gouy-Pailler Combining homomorphic encryption and differential privacy in federated learning 12:00-… — Lunch \u0026amp; Departure We look forward to welcoming you in Annecy for two days of scientific exchange and collaboration.\n","date":1758326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1758326400,"objectID":"ae833fb931530c2dc70a09ae1b8c67d4","permalink":"https://redeem-pepria.github.io/en/post/2025-09-24_retreat/","publishdate":"2025-09-20T00:00:00Z","relpermalink":"/en/post/2025-09-24_retreat/","section":"post","summary":"We are pleased to announce the REDEEM project retreat taking place in Annecy.\nThis two-day seminar will bring together project members and partners to present, exchange, and discuss ongoing research on decentralized, robust and privacy-preserving learning.\n","tags":null,"title":"REDEEM retreat in Annecy","type":"post"},{"authors":["Pierre Jobic","Aurélien Mayoue","Sara Tucci Piergiovanni"],"categories":null,"content":"","date":1756080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756080000,"objectID":"e5473e41180d307fab7690cd4a1af77b","permalink":"https://redeem-pepria.github.io/en/publication/jobic-regularizing-gradient-reconstruction-2025-a/","publishdate":"2025-10-16T21:33:42.484704Z","relpermalink":"/en/publication/jobic-regularizing-gradient-reconstruction-2025-a/","section":"publication","summary":"Federated Learning (FL) has gained prominence as a decentralized and privacy-preserving paradigm that enables multiple clients to collaboratively train a machine learning model under the supervision of a central server but without sharing their data. By design, FL is a solution for data privacy but not model privacy. Recent attacks, such as gradient reconstruction attacks (GRAs) have precisely shown privacy issues when an attacker knows the model parameters sent by a client to the server. In the literature, these privacy issues are mainly explored when clients compute a single gradient descent step on their data (FedSGD). In a more realistic scenario, clients compute several gradient descent steps (FedAvg). This protocol adds intermediate computation steps, which are unknown from the attacker, thus making GRAs less successful. In this paper, we introduce a new regularizer that makes GRAs more efficient under FedAvg. Our discussion is supported with experiments in computer vision.","tags":null,"title":"Regularizing Gradient Reconstruction Attacks in Federated Averaging","type":"publication"},{"authors":["Justine Cauvi","Laurent Viennot"],"categories":null,"content":"","date":1751932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751932800,"objectID":"1c7d3c8f42efb56a74fc75145310adbb","permalink":"https://redeem-pepria.github.io/en/publication/cauvi-parameterized-restless-temporal-2025/","publishdate":"2025-10-16T21:33:42.344669Z","relpermalink":"/en/publication/cauvi-parameterized-restless-temporal-2025/","section":"publication","summary":"Recently, Bumpus and Meeks introduced a purely temporal parameter, called vertex-interval-membership-width, which is promising for the design of fixed-parameter tractable (FPT) algorithms for vertex reachability problems in temporal graphs. We study this newly introduced parameter for the problem of restless temporal paths, in which the waiting time at each node is restricted. In this article, we prove that, in the interval model, where arcs are present for entire time intervals, finding a restless temporal path is NP-hard even if the vertex-interval-membership-width is equal to three. We exhibit FPT algorithms for the point model, where arcs are present at specific points in time, both with uniform delay one and arbitrary positive delays. In the latter case, this comes with a slight additional computational cost.","tags":["Computer Science - Computational Complexity","Computer Science - Data Structures and Algorithms"],"title":"Parameterized Restless Temporal Path","type":"publication"},{"authors":["Constantin Philippenko","Batiste Le Bars","Kevin Scaman","Laurent Massoulie"],"categories":null,"content":"","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751328000,"objectID":"2f4e074308fa2a93ea95970ce5c900d7","permalink":"https://redeem-pepria.github.io/en/publication/philippenko-adaptive-collaboration-online-2025/","publishdate":"2025-10-16T21:33:42.578351Z","relpermalink":"/en/publication/philippenko-adaptive-collaboration-online-2025/","section":"publication","summary":"We study the problem of online personalized decentralized learning with NNN statistically heterogeneous clients collaborating to accelerate local training. An important challenge in this setting is to select relevant collaborators to reduce gradient variance while mitigating the introduced bias. To tackle this, we introduce a gradient-based collaboration criterion, allowing each client to dynamically select peers with similar gradients during the optimization process. Our criterion is motivated by a refined and more general theoretical analysis of the  textttAll-for-one algorithm, proved to be optimal in Even et al. (2022) for an oracle collaboration scheme. We derive excess loss upper-bounds for smooth objective functions, being either strongly convex, non-convex, or satisfying the Polyak-Łojasiewicz condition; our analysis reveals that the algorithm acts as a variance reduction method where the speed-up depends on a  emphsufficient variance.  We put forward two collaboration methods instantiating the proposed general schema; and we show that one variant preserves the optimality of  textttAll-for-one.  We validate our results with experiments on synthetic and real datasets.","tags":["Federated Learning","Optimization","Personalization"],"title":"Adaptive Collaboration for Online Personalized Distributed Learning with Heterogeneous Clients","type":"publication"},{"authors":["Aymeric Dieuleveut","Gersende Fort","Mahmoud Hegazy","Hoi-To Wai"],"categories":null,"content":"","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751328000,"objectID":"d5e230351f6c98ed1ee800d5622d302d","permalink":"https://redeem-pepria.github.io/en/publication/dieuleveut-federated-majorize-minimization-parameter-2025/","publishdate":"2025-10-16T21:33:42.373227Z","relpermalink":"/en/publication/dieuleveut-federated-majorize-minimization-parameter-2025/","section":"publication","summary":"This paper proposes a unified approach for designing stochastic optimization algorithms that robustly scale to the federated learning setting. Our work studies a class of Majorize-Minimization (MM) problems, which possesses a linearly parameterized family of majorizing surrogate functions. This framework encompasses (proximal) gradient-based algorithms for (regularized) smooth objectives, the Expectation Maximization algorithm, and many problems seen as variational surrogate MM. We show that our framework motivates a unifying algorithm called Stochastic Approximation Stochastic Surrogate MM (SA-SSMM), which includes previous stochastic MM procedures as special instances. We then extend SA-SSMM to the federated setting, while taking into consideration common bottlenecks such as data heterogeneity, partial participation, and communication constraints; this yields FedMM. The originality of FedMM is to learn locally and then aggregate information characterizing the surrogate majorizing function, contrary to classical algorithms which learn and aggregate the original parameter. Finally, to showcase the flexibility of this methodology beyond our theoretical setting, we use it to design an algorithm for computing optimal transport maps in the federated setting.","tags":["Federated Learning","Stochastic Approximation","Stochastic Optimization","Surrogate Methods"],"title":"Federated Majorize-Minimization: Beyond Parameter Aggregation","type":"publication"},{"authors":["Yacine Belal","Mohamed Maouche","Sonia Ben Mokhtar","Anthony Simonet-Boulogne"],"categories":null,"content":"","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751328000,"objectID":"331f843b5a716a21bb960a6d6a51d45f","permalink":"https://redeem-pepria.github.io/en/publication/belal-inferring-communities-interest-2025/","publishdate":"2025-10-16T21:33:42.317218Z","relpermalink":"/en/publication/belal-inferring-communities-interest-2025/","section":"publication","summary":"Collaborative-learning-based recommender systems, such as those employing Federated Learning (FL) and Gossip Learning (GL), allow users to train models while keeping their history of liked items on their devices. While those methods were seen as promising for enhancing privacy, recent research has shown that collaborative learning can be vulnerable to various privacy attacks. In this paper, we propose a novel attack called Community Inference Attack (CIA), which enables an adversary to identify community members based on a set of target items. What sets CIA apart is its efficiency: it operates at a low computational cost by eliminating the need for training surrogate models. Instead, it uses a comparison-based approach, inferring sensitive information by comparing users' models rather than targeting any specific individual model. To evaluate the effectiveness of CIA, we conduct experiments on three real-world recommendation datasets using two recommendation models under both fedarated and gossip-like settings. The results demonstrate that CIA can be up to 10 times more accurate than random guessing. Additionally, we evaluate two mitigation strategies: Differentially Private Stochastic Gradient Descent (DP-SGD) and a Share less policy, which involves sharing fewer, less sensitive model parameters. Our findings suggest that the Share less strategy offers a better privacy-utility trade-off, especially in GL.","tags":["Collaborative Learning","Cryptography and Security (cs.CR)","Information Retrieval (cs.IR)","Privacy Attacks","Recommendation Systems"],"title":"Inferring Communities of Interest in Collaborative Learning-based Recommender Systems","type":"publication"},{"authors":["Brandon Alejandro Mosqueda González","Omar Hasan","Lionel Brunie"],"categories":null,"content":"","date":1748736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748736000,"objectID":"b06f44ef0726694ea5151864f2892e5a","permalink":"https://redeem-pepria.github.io/en/publication/mosquedagonzalez-mitigation-sybilbased-poisoning-2025-a/","publishdate":"2025-10-16T21:33:42.539486Z","relpermalink":"/en/publication/mosquedagonzalez-mitigation-sybilbased-poisoning-2025-a/","section":"publication","summary":"Decentralized learning enables collaborative machine learning with enhanced privacy by allowing participants to train models locally and share updates for aggregation instead of sharing raw data. However, such systems are vulnerable to poisoning attacks that may compromise the learning process. This threat becomes even more severe when combined with sybil attacks, where adversaries contribute numerous malicious updates with minimal effort. To overcome this challenge, particularly in the permissionless setup, we propose SyDeLP, a blockchain-enabled protocol for decentralized learning. SyDeLP integrates byzantine tolerant aggregation for poisoning mitigation with a Verifiable Delay Function to counter sybil attacks requiring Proofs of Work (PoW) to participate. Honest behavior is incentivized by dynamically reducing PoW difficulty, decreasing the computational burden for honest nodes over time. Empirical evaluations conducted on a benchmark dataset across three types of poisoning attacks demonstrate that SyDeLP consistently outperforms existing solutions in terms of resilience.","tags":["Adaptive Difficulty","Blockchain","Decentralized Learning","Permissionless","Verifiable Delay Function"],"title":"Mitigation of Sybil-based Poisoning Attacks in Permissionless Decentralized Learning","type":"publication"},{"authors":["Cédric Gouy-Pailler"],"categories":null,"content":"","date":1747353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747353600,"objectID":"fce8ed605fc8d97fccd01e8ddc98a36f","permalink":"https://redeem-pepria.github.io/en/publication/gouy-pailler-contributions-robustesse-securite-2025/","publishdate":"2025-10-16T21:33:42.45207Z","relpermalink":"/en/publication/gouy-pailler-contributions-robustesse-securite-2025/","section":"publication","summary":"","tags":null,"title":"Contributions à la robustesse, la sécurité et la confidentialité en apprentissage statistique","type":"publication"},{"authors":["Rebecca Clain","Eduardo Fernandes Montesuma","Fred Maurice Ngole Mboula"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"100a80d149495830bf1541e6322f738f","permalink":"https://redeem-pepria.github.io/en/publication/clain-decentralized-federated-dataset-2025/","publishdate":"2025-10-16T21:33:42.352321Z","relpermalink":"/en/publication/clain-decentralized-federated-dataset-2025/","section":"publication","summary":"","tags":["CEA-distrAI"],"title":"Decentralized Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation","type":"publication"},{"authors":["Aleksei Korneev","Jan Ramon"],"categories":null,"content":"","date":1742428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1742428800,"objectID":"e062ca147b950a24e8d7e0731ab75ce5","permalink":"https://redeem-pepria.github.io/en/publication/korneev-survey-verifiable-cross-silo-2025/","publishdate":"2025-10-16T21:33:42.496068Z","relpermalink":"/en/publication/korneev-survey-verifiable-cross-silo-2025/","section":"publication","summary":"Federated Learning (FL) is a widespread approach that allows training machine learning (ML) models with data distributed across multiple storage units. In cross-silo FL, which often appears in domains like healthcare or finance, the number of participants is moderate, and each party typically represents a well-known organization. For instance, in medicine data owners are often hospitals or data hubs which are well-established entities. However, malicious parties may still attempt to disturb the training procedure in order to obtain certain benefits, for example, a biased result or a reduction in computational load. While one can easily detect a malicious agent when data used for training is public, the problem becomes much more acute when it is necessary to maintain the privacy of the training dataset. To address this issue, there is recently growing interest in developing verifiable protocols, where one can check that parties do not deviate from the training procedure and perform computations correctly. In this paper, we present a survey on verifiable cross-silo FL. We analyze various protocols, fit them in a taxonomy, and compare their efficiency and threat models. We also analyze Zero-Knowledge Proof (ZKP) schemes and discuss how their overall cost in a FL context can be minimized. Lastly, we identify research gaps and discuss potential directions for future scientific work.","tags":null,"title":"A Survey on Verifiable Cross-Silo Federated Learning","type":"publication"},{"authors":["Cédric Gouy-Pailler"],"categories":null,"content":"REDEEM at PEPR IA Days (March 18-20, 2025) We are pleased to announce our participation at the upcoming PEPR IA Days (March 18-20, 2025).\nWe’ll proudly showcase some exciting first results from REDEEM! Check out the full program here: PEPR IA Days Program.\nOur team will feature:\n🔹 Rebecca Clain (CEA LIST) – Decentralized domain adaptation through optimal transport\n🔹 Constantin Philippenko (INRIA Paris) – In-depth Analysis of Low-rank Matrix Factorisation in a Federated Setting (AAAI 2025)\n🔹 Aymeric Dieuleveut (École Polytechnique) – Unified Breakdown Analysis for Byzantine Robust Gossip\nAdditionally, we’ll present the following posters:\nPierre Jobic (CEA LIST) – Leakage in Regression Federated Learning using Cryptographic Tools\nJulien Nicolas (CNRS LIRIS) – Differentially private and decentralized randomized power method\nOusmane Touat (CNRS LIRIS) – Scrutinizing the Vulnerability of Decentralized Learning to Membership Inference Attacks\nRenaud Gaucher (École Polytechnique) – Byzantine robust gossip algorithms\nWe look forward to welcoming you and discussing our research!\n#FederatedAI #PEPRIA #REDEEMProject #DistributedAI #Privacy #MLOps #Cybersecurity\n","date":1742256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1742256000,"objectID":"5dc8da68dc8988c36a45fa9a15ef122e","permalink":"https://redeem-pepria.github.io/en/post/2025-03-18_pepria-days/","publishdate":"2025-03-18T00:00:00Z","relpermalink":"/en/post/2025-03-18_pepria-days/","section":"post","summary":"REDEEM at PEPR IA Days (March 18-20, 2025) We are pleased to announce our participation at the upcoming PEPR IA Days (March 18-20, 2025).\nWe’ll proudly showcase some exciting first results from REDEEM! Check out the full program here: PEPR IA Days Program.\n","tags":null,"title":"PEPR IA Days","type":"post"},{"authors":["Cédric Gouy-Pailler"],"categories":null,"content":"REDEEM Project at “Les Rencontres de l’IA Embarquée” – March 7, 2025 On March 7, 2025, the REDEEM project participated in Les Rencontres de l’IA Embarquée, hosted at Bercy. Cedric, coordinator of the REDEEM project, co-hosted a matchmaking session titled “Distributed and Federated AI: From Edge to Cloud” alongside Hugo Miralles from MANTA (Inria Startup Studio).\nThe session focused on topics such as privacy, personalization, and MLOps, discussing alignment between the REDEEM project, the PEPR IA initiative, and other France 2030 initiatives.\nHugo Miralles presented the MANTA platform, explaining how it facilitates the deployment of federated and distributed AI solutions, emphasizing aspects like security, scalability, and observability. Participants shared their use cases, highlighting practical applications and challenges.\n#DistributedAI #FederatedAI #EdgeComputing #MLOps #EmbeddedAI #Innovation #REDEEMProject #PEPRIA\n","date":1741305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1741305600,"objectID":"51450fd513ce7101fab744961b46912d","permalink":"https://redeem-pepria.github.io/en/post/2025-03-07_aidge/","publishdate":"2025-03-07T00:00:00Z","relpermalink":"/en/post/2025-03-07_aidge/","section":"post","summary":"REDEEM Project at “Les Rencontres de l’IA Embarquée” – March 7, 2025 On March 7, 2025, the REDEEM project participated in Les Rencontres de l’IA Embarquée, hosted at Bercy. Cedric, coordinator of the REDEEM project, co-hosted a matchmaking session titled “Distributed and Federated AI: From Edge to Cloud” alongside Hugo Miralles from MANTA (Inria Startup Studio).\n","tags":null,"title":"REDEEM Project at Les Rencontres de l’IA Embarquée","type":"post"},{"authors":["Cédric Gouy-Pailler"],"categories":null,"content":"TRUMPET and REDEEM Cross-Fertilization Meeting On February 19, 2025, the TRUMPET project and the REDEEM initiative held a cross-fertilization meeting. This event allowed experts from both projects to:\nExchange insights on emerging ideas and innovative strategies in federated and decentralized approaches in artificial intelligence. Foster interdisciplinary cooperation to drive forward research, especially about intrication on legal and research aspects. ","date":1739923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1739923200,"objectID":"b33dcfd2d5282681f0c0729e6b4dfaf8","permalink":"https://redeem-pepria.github.io/en/post/2025-02-19_trumpet-redeem/","publishdate":"2025-02-19T00:00:00Z","relpermalink":"/en/post/2025-02-19_trumpet-redeem/","section":"post","summary":"TRUMPET and REDEEM Cross-Fertilization Meeting On February 19, 2025, the TRUMPET project and the REDEEM initiative held a cross-fertilization meeting. This event allowed experts from both projects to:\nExchange insights on emerging ideas and innovative strategies in federated and decentralized approaches in artificial intelligence. Foster interdisciplinary cooperation to drive forward research, especially about intrication on legal and research aspects. ","tags":null,"title":"TRUMPET and REDEEM cross-fertilization meeting","type":"post"},{"authors":["Cédric Gouy-Pailler"],"categories":null,"content":"Conference AI, Science, and Society\nWe are excited to share that the REDEEM project will be actively represented at the upcoming AI Conference 2025 at IP Paris!\nSession on Trustworthy AI Aymeric Dieuleveut will contribute to the discussion on trustworthy AI, providing insights into the latest advancements in secure and reliable AI systems.\nPoster Session Batiste Le Bars (INRIA) will present his work on “Improved Stability and Generalization Guarantees of the Decentralized SGD Algorithm.”\nThis research challenges prior claims on decentralized learning, demonstrating that decentralized stochastic gradient descent (SGD) can match centralized approaches in generalization while benefiting from communication-aware optimizations.\nJoin us at AI Conference 2025 to engage with cutting-edge research in AI, federated learning, and decentralized optimization!\n","date":1738627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738627200,"objectID":"88806b93c3d2ce729873bc904789d684","permalink":"https://redeem-pepria.github.io/en/post/2025-02-04_aiactionsummit/","publishdate":"2025-02-04T00:00:00Z","relpermalink":"/en/post/2025-02-04_aiactionsummit/","section":"post","summary":"Conference AI, Science, and Society\nWe are excited to share that the REDEEM project will be actively represented at the upcoming AI Conference 2025 at IP Paris!\nSession on Trustworthy AI Aymeric Dieuleveut will contribute to the discussion on trustworthy AI, providing insights into the latest advancements in secure and reliable AI systems.\n","tags":null,"title":"Join us at AI Action Summit 6-7/02/2025 at Ecole Polytechnique","type":"post"},{"authors":["Constantin Philippenko","Kevin Scaman","Laurent Massoulié"],"categories":null,"content":"","date":1738368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738368000,"objectID":"f72fe3535291cf7530d222a3dd4a225b","permalink":"https://redeem-pepria.github.io/en/publication/philippenko-indepth-analysis-lowrank-2025/","publishdate":"2025-10-16T21:33:42.588434Z","relpermalink":"/en/publication/philippenko-indepth-analysis-lowrank-2025/","section":"publication","summary":"We analyze a distributed algorithm to compute a low-rank matrix factorization on NNN clients, each holding a local dataset Si∈Rni×dSi∈Rni×d mathbfStextasciicircum i  in  mathbbRtextasciicircumn_i  times d, mathematically, we seek to solve minUi∈Rni×r,V∈Rd×r12∑Ni=1∥Si−UiV⊤∥2FminUi∈Rni×r,V∈Rd×r12∑i=1N‖Si−UiV⊤‖F2min_ mathbfUtextasciicircum i  in  mathbbRtextasciicircumn_i times r,  mathbfV in  mathbbRtextasciicircumd  times r   frac12  sum_i=1textasciicircum N  | mathbfStextasciicircum i -  mathbfUtextasciicircum i  mathbfVtextasciicircum top |textasciicircum 2_ textF. Considering a power initialization of VV mathbfV, we rewrite the previous smooth non-convex problem into a smooth strongly-convex problem that we solve using a parallel Nesterov gradient descent potentially requiring a single step of communication at the initialization step. For any client iii in 1,…,N1,…,N1,  dots, N, we obtain a global VV mathbfV in Rd×rRd×r mathbbRtextasciicircumd  times r common to all clients and a local variable UiUi mathbfUtextasciicircum i in Rni×rRni×r mathbbRtextasciicircumn_i  times r. We provide a linear rate of convergence of the excess loss which depends on σmax/σrσmax/σr sigma_ max /  sigma_r, where σrσr sigma_r is the rthrthrtextasciicircum mathrmth singular value of the concatenation SS mathbfS of the matrices (Si)Ni=1(Si)i=1N( mathbfStextasciicircum i)_i=1textasciicircum N. This result improves the rates of convergence given in the literature, which depend on σ2max/σ2minσmax2/σmin2 sigma_ maxtextasciicircum 2 /  sigma_ mintextasciicircum 2. We provide an upper bound on the Frobenius-norm error of reconstruction under the power initialization strategy. We complete our analysis with experiments on both synthetic and real data.","tags":["Federated learning","Matrix factorisation","Optimization"],"title":"In-Depth Analysis of Low-rank Matrix Factorisation in a Federated Setting","type":"publication"},{"authors":["Julien Nicolas"],"categories":null,"content":"Secure Federated Graph-Filtering for Recommender Systems\nRecommender systems rely on graph-based filters, but their centralized computation poses privacy and security risks. Our work introduces two decentralized frameworks that securely compute key graph components without centralizing user data. Leveraging Multi-Party Computation and distributed spectral methods, we ensure privacy while maintaining predictive accuracy. Our second approach further optimizes efficiency through low-rank approximations, balancing performance and communication costs. Experiments on benchmark datasets show that our methods match state-of-the-art accuracy while preserving data confidentiality. This work paves the way for privacy-preserving, decentralized recommender architectures.\nYou can already read this publication on arxiv: Secure Federated Graph-Filtering for Recommender Systems\nScientific Publications: Stay updated with the latest research outputs from our team, published in top-tier conferences and journals. Publications\n","date":1738108800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738108800,"objectID":"0e73420355239216986ac752617bdf0c","permalink":"https://redeem-pepria.github.io/en/post/2025-01-29_new-preprint-available/","publishdate":"2025-01-29T00:00:00Z","relpermalink":"/en/post/2025-01-29_new-preprint-available/","section":"post","summary":"Secure Federated Graph-Filtering for Recommender Systems\nRecommender systems rely on graph-based filters, but their centralized computation poses privacy and security risks. Our work introduces two decentralized frameworks that securely compute key graph components without centralizing user data. Leveraging Multi-Party Computation and distributed spectral methods, we ensure privacy while maintaining predictive accuracy. Our second approach further optimizes efficiency through low-rank approximations, balancing performance and communication costs. Experiments on benchmark datasets show that our methods match state-of-the-art accuracy while preserving data confidentiality. This work paves the way for privacy-preserving, decentralized recommender architectures.\n","tags":null,"title":"New Preprint Available","type":"post"},{"authors":null,"categories":null,"content":"We’re thrilled to announce that the new REDEEM Project website is officially live!\nExplore everything about the REDEEM project, including:\nProject Description: Learn about our mission to advance decentralized, privacy-preserving, and resilient AI systems. Project description Scientific Publications: Stay updated with the latest research outputs from our team, published in top-tier conferences and journals. Publications Job Opportunities: Want to work with us? Discover open positions in our team, including exciting PhD and postdoctoral opportunities. Job opportunities We’ll continue to update the site regularly with news, publications, and opportunities. Make sure to bookmark it and follow our journey!\nThank you for your support,\nThe REDEEM Project Team\n","date":1738022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738022400,"objectID":"1cdc6a246d8c9fc6edb0f9bf7a24cfde","permalink":"https://redeem-pepria.github.io/en/post/2025-01-28_new-website-live/","publishdate":"2025-01-28T00:00:00Z","relpermalink":"/en/post/2025-01-28_new-website-live/","section":"post","summary":"We’re thrilled to announce that the new REDEEM Project website is officially live!\nExplore everything about the REDEEM project, including:\nProject Description: Learn about our mission to advance decentralized, privacy-preserving, and resilient AI systems. Project description Scientific Publications: Stay updated with the latest research outputs from our team, published in top-tier conferences and journals. Publications Job Opportunities: Want to work with us? Discover open positions in our team, including exciting PhD and postdoctoral opportunities. Job opportunities We’ll continue to update the site regularly with news, publications, and opportunities. Make sure to bookmark it and follow our journey!\n","tags":null,"title":"Our New REDEEM Website is Live!","type":"post"},{"authors":null,"categories":null,"content":"","date":1738022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738022400,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://redeem-pepria.github.io/en/people/","publishdate":"2025-01-28T00:00:00Z","relpermalink":"/en/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":["Julien Nicolas","César Sabater","Mohamed Maouche","Sonia Ben Mokhtar","Mark Coates"],"categories":null,"content":"","date":1738022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738022400,"objectID":"e0e8d97b5d59cbcc8050eefc405fb874","permalink":"https://redeem-pepria.github.io/en/publication/nicolas-secure-federated-graph-filtering-2025/","publishdate":"2025-10-16T21:33:42.569334Z","relpermalink":"/en/publication/nicolas-secure-federated-graph-filtering-2025/","section":"publication","summary":"Recommender systems often rely on graph-based filters, such as normalized item-item adjacency matrices and low-pass filters. While effective, the centralized computation of these components raises concerns about privacy, security, and the ethical use of user data. This work proposes two decentralized frameworks for securely computing these critical graph components without centralizing sensitive information. The first approach leverages lightweight Multi-Party Computation and distributed singular vector computations to privately compute key graph filters. The second extends this framework by incorporating low-rank approximations, enabling a trade-off between communication efficiency and predictive performance. Empirical evaluations on benchmark datasets demonstrate that the proposed methods achieve comparable accuracy to centralized state-of-the-art systems while ensuring data confidentiality and maintaining low communication costs. Our results highlight the potential for privacy-preserving decentralized architectures to bridge the gap between utility and user data protection in modern recommender systems.","tags":["Computer Science - Cryptography and Security","Computer Science - Information Retrieval"],"title":"Secure Federated Graph-Filtering for Recommender Systems","type":"publication"},{"authors":["Feodor Dragan","Guillaume Ducoffe","Michel Habib","Laurent Viennot"],"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"92044013f6ef8ba1b09df8b393132d84","permalink":"https://redeem-pepria.github.io/en/publication/dragan-certificates-subquadratic-time-computation-2025/","publishdate":"2025-10-16T21:33:42.383661Z","relpermalink":"/en/publication/dragan-certificates-subquadratic-time-computation-2025/","section":"publication","summary":"In the context of fine-grained complexity, we investigate the notion of certificate enabling faster polynomialtime algorithms. We specifically target radius (minimum eccentricity), diameter (maximum eccentricity), and all-eccentricity computations for which quadratic-time lower bounds are known under plausible conjectures. In each case, we introduce a notion of certificate as a specific set of nodes from which appropriate bounds on all eccentricities can be derived in subquadratic time when this set has sublinear size. The existence of small certificates is a barrier against SETH-based lower bounds for these problems. We indeed prove that for graph classes with small certificates, there exist randomized subquadratic-time algorithms for computing the radius, the diameter, and all eccentricities respectively.Moreover, these notions of certificates are tightly related to algorithms probing the graph through one- to-all distance queries and allow to explain the efficiency of practical radius and diameter algorithms from the literature. Our formalization enables a novel primal-dual analysis of a classical approach for diameter computation that leads to algorithms for radius, diameter and all eccentricities with theoretical guarantees with respect to certain graph parameters. This is complemented by experimental results on various types of real-world graphs showing that these parameters appear to be low in practice. Finally, we obtain refined results for several graph classes.*This work was supported by the French ANR projects ANR-22-CE48-0001 (TEMPOGRAL), ANR-24-CE48-4377 (GODASse) and ANR-23-PEIA-005 (REDEEM).","tags":["INRIA-ARGO"],"title":"Certificates in P and Subquadratic-Time Computation of Radius, Diameter, and All Eccentricities in Graphs","type":"publication"},{"authors":null,"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"8af25929ca8895de4f7cefaa32947b2c","permalink":"https://redeem-pepria.github.io/en/joboffer/inria_janramon_engineer/","publishdate":"2025-01-01T00:00:00Z","relpermalink":"/en/joboffer/inria_janramon_engineer/","section":"joboffer","summary":"Follow the link to get more details on this opportunity.","tags":["engineer"],"title":"Engineer - Scientific programmer in privacy-preserving federate learning with applications in oncology","type":"joboffer"},{"authors":["Yacine Belal","Mohamed Maouche","Sonia Ben Mokhtar","Anthony Simonet-Boulogne"],"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"6d5f32fad97912b932ac3cb4884efe3a","permalink":"https://redeem-pepria.github.io/en/publication/belal-granite-byzantine-resilient-dynamic-2025-a/","publishdate":"2025-10-16T21:33:42.300887Z","relpermalink":"/en/publication/belal-granite-byzantine-resilient-dynamic-2025-a/","section":"publication","summary":"Gossip Learning (GL) is a decentralized learning paradigm where users iteratively exchange and aggregate models with a small set of neighboring peers. Recent GL approaches rely on dynamic communication graphs built and maintained using Random Peer Sampling (RPS) protocols. Thanks to graph dynamics, GL can achieve fast convergence even over extremely sparse topologies. However, the robustness of GL over dy- namic graphs to Byzantine (model poisoning) attacks remains unaddressed especially when Byzantine nodes attack the RPS protocol to scale up model poisoning. We address this issue by introducing GRANITE, a framework for robust learning over sparse, dynamic graphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two key components (i) a History-aware Byzantine-resilient Peer Sampling protocol (HaPS), which tracks previously encountered identifiers to reduce adversarial influence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which leverages an estimate of Byzantine presence to set aggregation thresholds with formal guarantees. Empirical results confirm that GRANITE maintains convergence with up to 30% Byzantine nodes, improves learning speed via adaptive filtering of poisoned models and obtains these results in up to 9 times sparser graphs than dictated by current theory.","tags":["Gossip learning","poisoning attacks","random peer sampling","robust aggregation"],"title":"GRANITE : A Byzantine-Resilient Dynamic Gossip Learning Framework","type":"publication"},{"authors":null,"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"7bc14dd430089e42e54f32d8f0b522b7","permalink":"https://redeem-pepria.github.io/en/joboffer/inria_janramon_interns/","publishdate":"2025-01-01T00:00:00Z","relpermalink":"/en/joboffer/inria_janramon_interns/","section":"joboffer","summary":"Contact Jan Ramon if you are interested.","tags":["intern"],"title":"Interships in privacy-preserving decentralized machine learning","type":"joboffer"},{"authors":["Brandon A. Mosqueda González","Omar Hasan","Lionel Brunie"],"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"6dd9d8b1c2d5645d3583239c66ff15f4","permalink":"https://redeem-pepria.github.io/en/publication/mosquedagonzalez-mitigation-sybilbased-poisoning-2025/","publishdate":"2025-01-28T21:08:55.802492Z","relpermalink":"/en/publication/mosquedagonzalez-mitigation-sybilbased-poisoning-2025/","section":"publication","summary":"Decentralized learning enables collaborative machine learning with enhanced privacy by allowing participants to train models locally and share updates for aggregation instead of sharing raw data. However, such systems are vulnerable to poisoning attacks that may compromise the learning process. This threat becomes even more severe when combined with sybil attacks, where adversaries contribute numerous malicious updates with minimal effort, amplifying their impact. To overcome these challenges, particularly in the permissionless setup, we propose SyDeLP, a blockchain-enabled protocol for decentralized learning. SyDeLP integrates byzantine tolerant aggregation for poisoning mitigation with a novel Verifiable Delay Puzzle to counter sybil attacks requiring Proofs of Work to participate. Honest behavior is incentivized by dynamically reducing puzzle difficulty, decreasing the computational burden for honest nodes over time. Empirical evaluations conducted on two benchmark datasets across four types of poisoning attack demonstrate that SyDeLP consistently outperforms existing solutions in terms of poisoning resilience.","tags":["CNRS-LIRIS"],"title":"Mitigation of Sybil-based Poisoning Attacks in Permissionless Decentralized Learning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"40cc3e5a0b716668fb694847d0c18408","permalink":"https://redeem-pepria.github.io/en/joboffer/inria_janramon_phd/","publishdate":"2025-01-01T00:00:00Z","relpermalink":"/en/joboffer/inria_janramon_phd/","section":"joboffer","summary":"See the link.","tags":["phd"],"title":"PhD Position F/M PhD student on privacy-preserving federate learning with applications in oncology","type":"joboffer"},{"authors":null,"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"54fbe2e4d15da53a91ecf56993b35204","permalink":"https://redeem-pepria.github.io/en/joboffer/inria_janramon_postdoc/","publishdate":"2025-01-01T00:00:00Z","relpermalink":"/en/joboffer/inria_janramon_postdoc/","section":"joboffer","summary":"We hope the post-doc can bring new expertise to the group and/or can help intensifying collaboration in the project consortium.  He will collaborate closely with the other group members on realizing the research objectives of the project.  Engineers in the team can support the prototyping and validation.","tags":["postdoc"],"title":"Post-Doctoral Research Visit F/M privacy preserving federated learning with applications in medical domains","type":"joboffer"},{"authors":["Ousmane Touat","Jezekael Brunon","Yacine Belal","Julien Nicolas","Mohamed Maouche","César Sabater","Sonia Ben Mokhtar"],"categories":null,"content":"","date":1734393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734393600,"objectID":"050c63c4d82f0e0eb0aac8839df01953","permalink":"https://redeem-pepria.github.io/en/publication/touat-scrutinizing-vulnerability-decentralized-2024/","publishdate":"2025-10-16T21:33:42.623859Z","relpermalink":"/en/publication/touat-scrutinizing-vulnerability-decentralized-2024/","section":"publication","summary":"The primary promise of decentralized learning is to allow users to engage in the training of machine learning models in a collaborative manner while keeping their data on their premises and without relying on any central entity. However, this paradigm necessitates the exchange of model parameters or gradients between peers. Such exchanges can be exploited to infer sensitive information about training data, which is achieved through privacy attacks (e.g Membership Inference Attacks -- MIA). In order to devise effective defense mechanisms, it is important to understand the factors that increase/reduce the vulnerability of a given decentralized learning architecture to MIA. In this study, we extensively explore the vulnerability to MIA of various decentralized learning architectures by varying the graph structure (e.g number of neighbors), the graph dynamics, and the aggregation strategy, across diverse datasets and data distributions. Our key finding, which to the best of our knowledge we are the first to report, is that the vulnerability to MIA is heavily correlated to (i) the local model mixing strategy performed by each node upon reception of models from neighboring nodes and (ii) the global mixing properties of the communication graph. We illustrate these results experimentally using four datasets and by theoretically analyzing the mixing properties of various decentralized architectures. Our paper draws a set of lessons learned for devising decentralized learning systems that reduce by design the vulnerability to MIA.","tags":["CNRS-LIRIS"],"title":"Scrutinizing the Vulnerability of Decentralized Learning to Membership Inference Attacks","type":"publication"},{"authors":["Paul Mangold","Alain Durmus","Aymeric Dieuleveut","Sergey Samsonov","Eric Moulines"],"categories":null,"content":"","date":1733097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733097600,"objectID":"bc5d10cfd7f60d1891a315dcf554e41f","permalink":"https://redeem-pepria.github.io/en/publication/mangold-refined-analysis-federated-2024/","publishdate":"2025-10-16T21:33:42.528827Z","relpermalink":"/en/publication/mangold-refined-analysis-federated-2024/","section":"publication","summary":"In this paper, we present a novel analysis of FedAvg with constant step size, relying on the Markov property of the underlying process. We demonstrate that the global iterates of the algorithm converge to a stationary distribution and analyze its resulting bias and variance relative to the problem's solution. We provide a first-order expansion of the bias in both homogeneous and heterogeneous settings. Interestingly, this bias decomposes into two distinct components: one that depends solely on stochastic gradient noise and another on client heterogeneity. Finally, we introduce a new algorithm based on the Richardson-Romberg extrapolation technique to mitigate this bias.","tags":["X"],"title":"Refined Analysis of Federated Averaging's Bias and Federated Richardson-Romberg Extrapolation","type":"publication"},{"authors":["Julien Nicolas","César Sabater","Mohamed Maouche","Sonia Ben Mokhtar","Mark Coates"],"categories":null,"content":"","date":1732579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732579200,"objectID":"3e6b5d16fa1f7c6b9d5d05ebe7ee963d","permalink":"https://redeem-pepria.github.io/en/publication/nicolas-differentially-private-decentralized-2024/","publishdate":"2025-10-16T21:33:42.548262Z","relpermalink":"/en/publication/nicolas-differentially-private-decentralized-2024/","section":"publication","summary":"The randomized power method has gained significant interest due to its simplicity and efficient handling of large-scale spectral analysis and recommendation tasks. As modern datasets contain sensitive private information, we need to give formal guarantees on the possible privacy leaks caused by this method. This paper focuses on enhancing privacy preserving variants of the method. We propose a strategy to reduce the variance of the noise introduced to achieve Differential Privacy (DP). We also adapt the method to a decentralized framework with a low computational and communication overhead, while preserving the accuracy. We leverage Secure Aggregation (a form of Multi-Party Computation) to allow the algorithm to perform computations using data distributed among multiple users or devices, without revealing individual data. We show that it is possible to use a noise scale in the decentralized setting that is similar to the one in the centralized setting. We improve upon existing convergence bounds for both the centralized and decentralized versions. The proposed method is especially relevant for decentralized applications such as distributed recommender systems, where privacy concerns are paramount.","tags":["CNRS-LIRIS"],"title":"Differentially Private and Decentralized Randomized Power Method","type":"publication"},{"authors":["Renaud Gaucher","Aymeric Dieuleveut","Hadrien Hendrikx"],"categories":null,"content":"","date":1728864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728864000,"objectID":"f646ebc72c834309ac09b556aaa68544","permalink":"https://redeem-pepria.github.io/en/publication/gaucher-achieving-optimal-breakdown-2024/","publishdate":"2025-10-16T21:33:42.418951Z","relpermalink":"/en/publication/gaucher-achieving-optimal-breakdown-2024/","section":"publication","summary":"Distributed approaches have many computational benefits, but they are vulnerable to attacks from a subset of devices transmitting incorrect information. This paper investigates Byzantine-resilient algorithms in a decentralized setting, where devices communicate directly with one another. We investigate the notion of breakdown point, and show an upper bound on the number of adversaries that decentralized algorithms can tolerate. We introduce $ mathrmCGtextasciicircum +$, an algorithm at the intersection of $ mathrmClippedGossip$ and $ mathrmNNA$, two popular approaches for robust decentralized learning. $ mathrmCGtextasciicircum +$ meets our upper bound, and thus obtains optimal robustness guarantees, whereas neither of the existing two does. We provide experimental evidence for this gap by presenting an attack tailored to sparse graphs which breaks $ mathrmNNA$ but against which $ mathrmCGtextasciicircum +$ is robust.","tags":["X"],"title":"Achieving Optimal Breakdown for Byzantine Robust Gossip","type":"publication"},{"authors":["Constantin Philippenko","Kevin Scaman","Laurent Massoulié"],"categories":null,"content":"","date":1726185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726185600,"objectID":"0380a55c1c6edc1cbe09f546a4ef24d5","permalink":"https://redeem-pepria.github.io/en/publication/philippenko-indepth-analysis-lowrank-2024/","publishdate":"2025-01-28T21:08:55.832912Z","relpermalink":"/en/publication/philippenko-indepth-analysis-lowrank-2024/","section":"publication","summary":"We analyze a distributed algorithm to compute a low-rank matrix factorization on $N$ clients, each holding a local dataset $ mathbfStextasciicircum i  in  mathbbRtextasciicircumn_i  times d$, mathematically, we seek to solve $min_ mathbfUtextasciicircum i  in  mathbbRtextasciicircumn_i times r,  mathbfV in  mathbbRtextasciicircumd  times r   frac12  sum_i=1textasciicircum N  | mathbfStextasciicircum i -  mathbfUtextasciicircum i  mathbfVtextasciicircum top |textasciicircum 2_ textF$. Considering a power initialization of $ mathbfV$, we rewrite the previous smooth non-convex problem into a smooth strongly-convex problem that we solve using a parallel Nesterov gradient descent potentially requiring a single step of communication at the initialization step. For any client $i$ in $1,  dots, N$, we obtain a global $ mathbfV$ in $ mathbbRtextasciicircumd  times r$ common to all clients and a local variable $ mathbfUtextasciicircum i$ in $ mathbbRtextasciicircumn_i  times r$. We provide a linear rate of convergence of the excess loss which depends on $ sigma_ max /  sigma_r$, where $ sigma_r$ is the $rtextasciicircum mathrmth$ singular value of the concatenation $ mathbfS$ of the matrices $( mathbfStextasciicircum i)_i=1textasciicircum N$. This result improves the rates of convergence given in the literature, which depend on $ sigma_ maxtextasciicircum 2 /  sigma_ mintextasciicircum 2$. We provide an upper bound on the Frobenius-norm error of reconstruction under the power initialization strategy. We complete our analysis with experiments on both synthetic and real data.","tags":["INRIA-ARGO"],"title":"In-Depth Analysis of Low-rank Matrix Factorisation in a Federated Setting","type":"publication"},{"authors":["Batiste Le Bars","Aurélien Bellet","Marc Tommasi","Kevin Scaman","Giovanni Neglia"],"categories":null,"content":"","date":1721520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721520000,"objectID":"84184af5518df1a268fc37fb499166f5","permalink":"https://redeem-pepria.github.io/en/publication/lebars-improved-stability-generalization-2024/","publishdate":"2025-10-16T21:33:42.506566Z","relpermalink":"/en/publication/lebars-improved-stability-generalization-2024/","section":"publication","summary":"This paper presents a new generalization error analysis for Decentralized Stochastic Gradient Descent (D-SGD) based on algorithmic stability. The obtained results overhaul a series of recent works that suggested an increased instability due to decentralization and a detrimental impact of poorly-connected communication graphs on generalization. On the contrary, we show, for convex, strongly convex and non-convex functions, that D-SGD can always recover generalization bounds analogous to those of classical SGD, suggesting that the choice of graph does not matter. We then argue that this result is coming from a worst-case analysis, and we provide a refined optimization-dependent generalization bound for general convex functions. This new bound reveals that the choice of graph can in fact improve the worst-case bound in certain regimes, and that surprisingly, a poorly-connected graph can even be beneficial for generalization.","tags":["INRIA-ARGO","INRIA-MAGNET"],"title":"Improved Stability and Generalization Guarantees of the Decentralized SGD Algorithm","type":"publication"},{"authors":["Rémi Leluc","Aymeric Dieuleveut","François Portier","Johan Segers","Aigerim Zhuman"],"categories":null,"content":"","date":1720396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720396800,"objectID":"bee954922ceea9c15ee608a066cddfff","permalink":"https://redeem-pepria.github.io/en/publication/leluc-sliced-wasserstein-estimation-spherical-2024-a/","publishdate":"2025-10-16T21:33:42.517898Z","relpermalink":"/en/publication/leluc-sliced-wasserstein-estimation-spherical-2024-a/","section":"publication","summary":"The Sliced-Wasserstein (SW) distance between probability measures is defined as the average of the Wasserstein distances resulting for the associated one-dimensional projections. As a consequence, the SW distance can be written as an integral with respect to the uniform measure on the sphere and the Monte Carlo framework can be employed for calculating the SW distance. Spherical harmonics are polynomials on the sphere that form an orthonormal basis of the set of square-integrable functions on the sphere. Putting these two facts together, a new Monte Carlo method, hereby referred to as Spherical Harmonics Control Variates (SHCV), is proposed for approximating the SW distance using spherical harmonics as control variates. The resulting approach is shown to have good theoretical properties, e.g., a no-error property for Gaussian measures under a certain form of linear dependency between the variables. Moreover, an improved rate of convergence, compared to Monte Carlo, is established for general measures. The convergence analysis relies on the Lipschitz property associated to the SW integrand. Several numerical experiments demonstrate the superior performance of SHCV against state-of-the-art methods for SW distance computation.","tags":null,"title":"Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates","type":"publication"},{"authors":["Pierre Jobic","Aurélien Mayoue","Sara Tucci-Piergiovanni","François Terrier"],"categories":null,"content":"","date":1719187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719187200,"objectID":"2d2da173f8132d8b7926dcf5efe4174f","permalink":"https://redeem-pepria.github.io/en/publication/jobic-extending-scope-gradient-2024-a/","publishdate":"2025-10-16T21:33:42.470255Z","relpermalink":"/en/publication/jobic-extending-scope-gradient-2024-a/","section":"publication","summary":"Federated Learning (FL) has gained prominence as a decentralized and privacy-preserving paradigm that enables multiple clients to collaboratively train a machine learning model under the supervision of a central server. Instead of centralizing the data, clients keep their data locally and share only model parameters during multiple communication rounds. However, recent attacks, such as gradient reconstruction attacks (GRAs) show privacy issues when an attacker knows the communication of a client. In the literature, these privacy issues are mainly explored when clients compute new parameters using a single gradient descent step on their data (FedSGD) and then send them back to the remote server. In a more realistic scenario, the clients' protocol is based on several gradient descent steps (FedAvg). This protocol adds intermediate computation steps, which are unknown from the attacker, thus making GRAs less successful. In this incremental paper, we conduct exhaustive experiments on four state-of-the-art attacks under the FedAvg protocol, on a very basic and a more complex neural network (ResNet-18) with CIFAR100 dataset. These experiments provide the following results 1) a privacy-utility trade-off analysis, 2) insights on the choice of attacks' hyperparameters, 3) the client's local learning rate has little impact on attacks' effectiveness 4) a proof that the privacy risk is not necessarily decreasing over rounds, contrary to common belief.","tags":["CEA-distrAI"],"title":"Extending the Scope of Gradient Reconstruction Attacks in Federated Averaging","type":"publication"},{"authors":["Imane Taibi","Jan Ramon"],"categories":null,"content":"","date":1719187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719187200,"objectID":"0adace4ea82930a11bc6e9c5458732b5","permalink":"https://redeem-pepria.github.io/en/publication/taibi-honest-fraction-differential-2024/","publishdate":"2025-10-16T21:33:42.615137Z","relpermalink":"/en/publication/taibi-honest-fraction-differential-2024/","section":"publication","summary":"Over the last decades, differential privacy (DP) has become a standard notion of privacy. It allows to measure how much sensitive information an adversary could infer from a result (statistical model, prediction, etc.) he obtains. In privacy-preserving federated machine learning, one aims to learn a statistical model from data owned by multiple data owners without revealing their sensitive data. A common strategy is to use secure multi-party computation (SMPC) to avoid revealing intermediate results. However, DP assumes a very strong adversary who is able to know all information in the dataset except the targeted secret, while most SMPC methods assume a clearly less strong adversary, e.g., it is common to assume that the adversary has bounded computational power and can corrupt only a minority of the data owners (honest majority). As a chain is not stronger than its weakest part, in such combinations the DP provides an overly strong protection at an unnecessarily high cost in terms of utility. We propose honest fraction differential privacy, which is similar to differential privacy but assumes that the adversary can only collude with data owners covering part of the data. This assumption is very similar to the assumptions made by many SMPC strategies. We illustrate this idea by considering the application to the specific task of unregularized linear regression without bias on sufficiently large datasets.","tags":null,"title":"Honest Fraction Differential Privacy","type":"publication"},{"authors":["Renaud Gaucher","Hadrien Hendrikx","Aymeric Dieuleveut"],"categories":null,"content":"","date":1714953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714953600,"objectID":"378cb47ce6e302b69db03dd8e8404b82","permalink":"https://redeem-pepria.github.io/en/publication/gaucher-byzantine-robust-gossip-insights-2024/","publishdate":"2025-10-16T21:33:42.430612Z","relpermalink":"/en/publication/gaucher-byzantine-robust-gossip-insights-2024/","section":"publication","summary":"Distributed approaches have many computational benefits, but they are vulnerable to attacks from a subset of devices transmitting incorrect information. This paper investigates Byzantine-resilient algorithms in a decentralized setting, where devices communicate directly with one another. We leverage the so-called dual approach to design a general robust decentralized optimization method. We provide both global and local clipping rules in the special case of average consensus, with tight convergence guarantees. These clipping rules are practical, and yield results that finely characterize the impact of Byzantine nodes, highlighting for instance a qualitative difference in convergence between global and local clipping thresholds. Lastly, we demonstrate that they can serve as a basis for designing efficient attacks.","tags":["X"],"title":"Byzantine-Robust Gossip: Insights from a Dual Approach","type":"publication"},{"authors":["Mahmoud Hegazy","Rémi Leluc","Cheuk Ting Li","Aymeric Dieuleveut"],"categories":null,"content":"","date":1713398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713398400,"objectID":"126be16d00fb8c8afe8ff697607edd26","permalink":"https://redeem-pepria.github.io/en/publication/hegazy-compression-exact-error-2024/","publishdate":"2025-10-16T21:33:42.462415Z","relpermalink":"/en/publication/hegazy-compression-exact-error-2024/","section":"publication","summary":"Compression schemes have been extensively used in Federated Learning (FL) to reduce the communication cost of distributed learning. While most approaches rely on a bounded variance assumption of the noise produced by the compressor, this paper investigates the use of compression and aggregation schemes that produce a specific error distribution, e.g., Gaussian or Laplace, on the aggregated data. We present and analyze different aggregation schemes based on layered quantizers achieving exact error distribution. We provide different methods to leverage the proposed compression schemes to obtain compression-for-free in differential privacy applications. Our general compression methods can recover and improve standard FL schemes with Gaussian perturbations such as Langevin dynamics and randomized smoothing.","tags":["X"],"title":"Compression with Exact Error Distribution for Federated Learning","type":"publication"},{"authors":["Kevin Scaman","Mathieu Even","Batiste Le Bars","Laurent Massoulie"],"categories":null,"content":"","date":1713398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713398400,"objectID":"e27874dabf721c07999e6e9d9002c96c","permalink":"https://redeem-pepria.github.io/en/publication/scaman-minimax-excess-risk-2024/","publishdate":"2025-10-16T21:33:42.605588Z","relpermalink":"/en/publication/scaman-minimax-excess-risk-2024/","section":"publication","summary":"In this paper, our aim is to analyse the generalization capabilities of first-order methods for statistical learning in multiple, different yet related, scenarios including supervised learning, transfer learning, robust learning and federated learning. To do so, we provide sharp upper and lower bounds for the minimax excess risk of strongly convex and smooth statistical learning when the gradient is accessed through partial observations given by a data-dependent oracle. This novel class of oracles can query the gradient with any given data distribution, and is thus well suited to scenarios in which the training data distribution does not match the target (or test) distribution. In particular, our upper and lower bounds are proportional to the smallest mean square error achievable by gradient estimators, thus allowing us to easily derive multiple sharp bounds in the aforementioned scenarios using the extensive literature on parameter estimation.","tags":["INRIA-ARGO"],"title":"Minimax Excess Risk of First-Order Methods for Statistical Learning with Data-Dependent Oracles","type":"publication"},{"authors":["Yann Fraboni","Martin Van Waerebeke","Kevin Scaman","Richard Vidal","Laetitia Kameni","Marco Lorenzi"],"categories":null,"content":"","date":1713398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713398400,"objectID":"a3b5038611e3918dcf9cbd6a01f65cdb","permalink":"https://redeem-pepria.github.io/en/publication/fraboni-sifu-sequential-informed-2024/","publishdate":"2025-10-16T21:33:42.406083Z","relpermalink":"/en/publication/fraboni-sifu-sequential-informed-2024/","section":"publication","summary":"Machine Unlearning (MU) is an increasingly important topic in machine learning safety, aiming at removing the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client’s contribution from a federated training routine. While several FU methods have been proposed, we currently lack a general approach providing formal unlearning guarantees to the FedAvg routine, while ensuring scalability and generalization beyond the convex assumption on the clients’ loss functions. We aim at filling this gap by proposing SIFU (Sequential Informed Federated Unlearning), a new FU method applying to both convex and non-convex optimization regimes. SIFU naturally applies to FedAvg without additional computational cost for the clients and provides formal guarantees on the quality of the unlearning task. We provide a theoretical analysis of the unlearning properties of SIFU, and practically demonstrate its effectiveness as compared to a panel of unlearning methods from the state-of-the-art.","tags":["INRIA-ARGO"],"title":"SIFU: Sequential Informed Federated Unlearning for Efficient and Provable Client Unlearning in Federated Optimization","type":"publication"},{"authors":["Fabiola Espinoza Castellon","Eduardo Fernandes Montesuma","Fred Maurice Ngole Mboula","Aurélien Mayoue","Antoine Souloumiac","Cedric Gouy-Pailler"],"categories":null,"content":"","date":1713052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713052800,"objectID":"56ac8000f9051dcfdffc854692c75ad6","permalink":"https://redeem-pepria.github.io/en/publication/castellon-federated-dataset-dictionary-2024/","publishdate":"2025-10-16T21:33:42.330256Z","relpermalink":"/en/publication/castellon-federated-dataset-dictionary-2024/","section":"publication","summary":"In this article, we propose an approach for federated domain adaptation, a setting where data heterogeneity exists among clients and some have unlabeled data. The proposed framework, FedDaDiL, tackles the resulting challenge through dictionary learning of empirical distributions. In our setting, clients' distributions represent particular domains, and FedDaDiL collectively trains a federated dictionary of empirical distributions. In particular, we build upon the Dataset Dictionary Learning framework by designing communication protocols and aggregation operations in a collaborative setting. The chosen protocols keep clients' data private, thus enhancing overall privacy compared to its centralized counterpart. We empirically demonstrate that our approach successfully labels the target domain with extensive experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks. Furthermore, we compare our method to its centralized counterpart and other benchmarks in federated domain adaptation.","tags":null,"title":"Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation","type":"publication"},{"authors":["Vitalii Emelianov","Michaël Perrot"],"categories":null,"content":"","date":1707091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707091200,"objectID":"52593d893d015e954e4d94dfc8e4e9f1","permalink":"https://redeem-pepria.github.io/en/publication/emelianov-impact-output-perturbation-2024/","publishdate":"2025-10-16T21:33:42.395418Z","relpermalink":"/en/publication/emelianov-impact-output-perturbation-2024/","section":"publication","summary":"We theoretically study how differential privacy interacts with both individual and group fairness in binary linear classification. More precisely, we focus on the output perturbation mechanism, a classic approach in privacy-preserving machine learning. We derive high-probability bounds on the level of individual and group fairness that the perturbed models can achieve compared to the original model. Hence, for individual fairness, we prove that the impact of output perturbation on the level of fairness is bounded but grows with the dimension of the model. For group fairness, we show that this impact is determined by the distribution of so-called angular margins, that is signed margins of the non-private model re-scaled by the norm of each example.","tags":["INRIA-MAGNET"],"title":"On the Impact of Output Perturbation on Fairness in Binary Linear Classification","type":"publication"}]